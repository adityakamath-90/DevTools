version: '3.8'

services:
  # DevTools Application Container
  devtools:
    build: .
    container_name: devtools-app
    volumes:
      # Mount input directory (read-only) - team puts Kotlin files here
      - ./input:/app/input:ro
      # Mount output directory (read-write) - generated files appear here
      - ./output:/app/output
      # Mount data directory for RAG (optional existing test cases)
      - ./data:/app/data:ro
      # Cache models for better performance
      - devtools_models:/app/models
    environment:
      # Configuration for AI model
      - PYTHONUNBUFFERED=1
      - OLLAMA_API_URL=http://ollama:11434/api/generate
      - MODEL_NAME=codellama:instruct
      # Ensure proper file encoding
      - LANG=C.UTF-8
      - LC_ALL=C.UTF-8
    depends_on:
      - ollama
    networks:
      - devtools-network
    profiles:
      - dev
      - production

  # Ollama AI Model Server Container
  ollama:
    image: ollama/ollama:latest
    container_name: devtools-ollama
    volumes:
      # Persist downloaded models across container restarts
      - devtools_ollama:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    ports:
      # Expose Ollama API (optional, for debugging)
      - "11434:11434"
    networks:
      - devtools-network
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 11434 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    # Download CodeLlama model on first startup
    entrypoint: ["/bin/bash", "-c"]
    command: >
      "ollama serve &
       sleep 20 &&
       ollama pull codellama:instruct &&
       wait"
    profiles:
      - dev
      - production

volumes:
  # Persistent storage for downloaded models
  devtools_ollama:
    name: devtools_ollama_models
  devtools_models:
    name: devtools_sentence_models

networks:
  devtools-network:
    driver: bridge
    name: devtools_network
